---
title: "DATA2002 Assignment"
author: "Jing Yang"
output:
  bookdown::html_document2: # note that this requires bookdown
    toc: yes
    toc_depth: 3
    toc_float: yes
    code_folding: hide
    number_sections: true
    theme: paper
    highlight: kate
    css: style.css
---

This R Markdown file walks through some data cleaning for the assignment survey data. It does not represent a complete set of data cleaning code, there are many variables that haven't been cleaned and there are some decisions you might take for individual rows to possibly exclude them. You're welcome to use any of the code below in your assignment, but you will almost certainly need to adapt it and add to it.

Your assignment is not about data cleaning, as such, but you will need to include the code that you use to clean your data and a couple of paragraphs describing what data cleaning was performed and justifying any decisions you made along the way.

You don't need to go step by step through the process like I've done below, a single "data cleaning" code chunk that performs all the cleaning at once and a couple of paragraphs describing the rationale are all that's required. 

I've put a bit of effort making the tables look nice and the plots are OK - they have captions, but the legends/colours/theming of some could still be tidied up, I wouldn't get 10/10 for this effort. Every figure or table you include in your assignment should also be referenced and discussed in the text. If you don't talk about a table/figure then it shouldn't be included. They don't "speak for themselves". [This is also something I haven't always done below, another reason that I wouldn't get 10/10.]

```{r setup, include=TRUE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(janitor)
library(skimr)
library(visdat)
library(gendercodeR)
```

# Import the data

```{r, message=FALSE}
# readr::read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vTf8eDSN_2QbMTyVvO5bdYKZSEEF2bufDdYnLnL-TsR8LM-6x-xu1cxmDMohlbLrkMJn9DE7EG7pg5P/pub?gid=1724783278&single=true&output=csv")
raw = read_csv("DATA2X02 class survey 2020 (Responses) - Form responses 1.csv")
```

## Clean column names

Here are the column names:

```{r}
tibble(Position = 1:21, `Column names` = colnames(raw)) %>%
  gt::gt() %>% 
  gt::tab_source_note("Table 1: Questions asked in the DATA2002 survey.")
```

First we'll clean the column names with the janitor package. 

```{r, results = "hide"}
x = raw %>% janitor::clean_names()
colnames(x)
```

```{r}
colnames(x) = stringr::str_replace(string = colnames(x),
                                   pattern = "what_is_your_",
                                   replacement = "")
colnames(x) = stringr::str_replace(string = colnames(x),
                                   pattern = "on_average_how_many_hours_per_week_did_you_",
                                   replacement = "")
```

In this case, it's probably easiest to go through and manually shorten them.

```{r}
colnames(x)[2] = "covid_test"
colnames(x)[4] = "postcode"
colnames(x)[5] = "dentist"
colnames(x)[6] = "university_work"
colnames(x)[7] = "social_media"
colnames(x)[8] = "dog_or_cat"
colnames(x)[9] = "live_with_parents"
colnames(x)[10] = "exercising"
colnames(x)[12] = "asthma"
colnames(x)[13] = "paid_work"
colnames(x)[14] = "fav_season"
colnames(x)[16] = "height"
colnames(x)[17] = "floss_frequency"
colnames(x)[18] = "glasses"
colnames(x)[20] = "steak_preference"
colnames(x)[21] = "stress_level"
```

```{r}
tibble(Position = 1:21, `Variable name` = colnames(x), `Corresponding question` = colnames(raw)) %>%
  gt::gt() %>% 
  gt::tab_source_note("Table 2: Variable names and the corresponding questions asked in the DATA2002 survey.")
```

Later on in the data cleaning process we identified that postcode isn't really a numeric variable, so we've come back and `mutate()`d it to a character vector here. Similarly we found that `timestamp` appears as a character variable. The **lubridate** package has been used to convert it to a date time object.


```{r}
x = x %>% mutate(
  postcode = as.character(postcode),
  timestamp = lubridate::dmy_hms(timestamp)
)
```

# Quick look at the data

It's always a good idea to generate some basic summary statistics and visualisations when you get a new data set. There are almost always issues in the data that need to be dealt with.

## skimr package

```{r}
x %>% skimr::skim()
```

## visdat package

We can visualise the missingness in the data using the **visdat** package. In Figure \@ref(fig:missingness) below we can see that some missingness exists and we should probably go back and remove the rows that are completely empty (apart from a timestamp). To do this you might consider using the `filter()` function or the more specific `drop_na()` function.

```{r missingness, fig.cap="Visualising the missingness in the data."}
visdat::vis_miss(x)
x = x %>% drop_na()
```


## Numeric variables

```{r, results = "hide", echo = FALSE}
# this code chunk and it's result is hidden from the HTML document
# x %>% dplyr::select(where(is.numeric)) %>% colnames()
```

Figure \@ref(fig:overviewplot) presents an overview of all the numeric variables before any cleaning has been preformed.

```{r overviewplot, message=FALSE, warning = FALSE, fig.cap="Histograms and bar charts for the numeric variables."}
p1 = x %>% ggplot(aes(x = covid_test)) + geom_bar()
p2 = x %>% ggplot(aes(x = stress_level)) + geom_bar()
p3 = x %>% ggplot(aes(x = university_work)) + geom_histogram()
p4 = x %>% ggplot(aes(x = exercising)) + geom_histogram()
p5 = x %>% ggplot(aes(x = paid_work)) + geom_histogram()
p6 = x %>% ggplot(aes(x = shoe_size)) + geom_histogram()
p7 = x %>% ggplot(aes(x = height)) + geom_histogram()
gridExtra::grid.arrange(p1, p2, p3, p4, p5, p6, p7, ncol = 3)
```

### Height

We can clean the height variable by making a reasonable guess that heights like 1.62 should be 162 and we implement this using the `case_when()` function.

```{r}
# table(x$height)
x = x %>% 
  dplyr::mutate(
    height = dplyr::case_when(
      height < 2.3 ~ height*100,
      # if there were also heights in feet we could use this next:
      # height < 7 ~ height* <conversion factor from feet to m>,
      TRUE ~ height
    )
  )
```


```{r, fig.height = 3, fig.cap = "Distribution of heights after rescaling the heights reported in metres."}
x %>% 
  ggplot(aes(x = height)) +
  geom_histogram() + 
  labs(x = "Height (cm)", y = "Count") + 
  theme_bw()
```

### Other continuous variables

Cleaning the other continuous variables is left as an exercise for the reader.

## Categorical variables

```{r, echo = FALSE, results = "hide"}
# x %>% select(where(is.character)) %>% colnames()
```

### Gender

Approach 2: using the [**gendercodeR** package](https://github.com/ropenscilabs/gendercoder) which is tailor made to help in this situation.

```{r}
x = x %>% mutate(
  gender = gendercodeR::recode_gender(gender)
)
x %>% 
  janitor::tabyl(gender) %>% 
  gt::gt() %>% 
  gt::fmt_percent(columns = c("percent"),
                  decimals = 0) %>% 
  gt::tab_source_note("Table 3: Summary of gender proportions.")
```

### Eye colour

With eye colour we've taken a different approach, using the `fct_lump()` function from the **forcats** package. See `fct_lump` for details.

```{r}
# x %>% janitor::tabyl(eye_colour)
# x %>% filter(eye_colour == "Yellow")
x = x %>% 
  mutate(
    eye_colour = tolower(eye_colour),
    eye_colour = forcats::fct_lump(eye_colour, n = 6)
  )
x %>% 
  janitor::tabyl(eye_colour) %>% 
  arrange(desc(n)) %>% # arrange in descending order
  gt::gt() %>% 
  gt::fmt_percent(columns = c("percent"),
                  decimals = 0) %>% 
  gt::tab_source_note("Table 4: Summary of eye colour proportions.")
```

### Exercising

```{r}
x %>% janitor::tabyl(exercising) %>%
arrange(desc(exercising)) %>% # arrange in descending order
  gt::gt() %>% 
  gt::fmt_percent(columns = c("percent"),
                  decimals = 0) %>% 
  gt::tab_source_note("Table 4: Summary of exercising.")
```

### Stress level

```{r}
x %>% janitor::tabyl(stress_level) %>%
arrange(stress_level) %>% # arrange in descending order
  gt::gt() %>% 
  gt::fmt_percent(columns = c("percent"),
                  decimals = 0) %>% 
  gt::tab_source_note("Table 4: Summary of stress level.")
```

### Steak preference

There's a natural ordering to the cooking preferences, we we'll make sure that's respected in Figure \@ref(fig:steakplot) by `mutate()`ing it to a factor and specifing the `levels` in the correct order.

```{r}
steak_levels = c("Rare", "Medium-rare", "Medium", 
                 "Medium-well done", "Well done", 
                 "I don't eat beef")
x = x %>% 
  mutate(
    steak_preference = factor(steak_preference, levels = steak_levels)
  )
```


```{r steakplot, fig.height=3, fig.cap="Distribution of how well cooked people prefered their steak."}
x %>% ggplot() + 
  aes(y = steak_preference, fill = steak_preference) + 
  geom_bar() + 
  theme(legend.position = "none") + 
  labs(y = "", x = "Count")
```

## Time variable

If you ever get a dataset with a time variable, make sure you check out the distribution of observations over time. You can often notice oddities or gaps in the data this way.

```{r, fig.cap="Distribution of responses over time."}
x %>% ggplot() + aes(x = timestamp) + 
  geom_histogram()
```


# Questions

## Is this a random sample of DATA2002 students?

A random sample is a sample chosen from a larger set. Each individual is chosen randomly and entirely by enchange, such that each individual has the same proability of being chosen. This is a voluntary survey. No students are forced to answer the survey therefore every DATA2002 student has the same probability to complete the survey at any stage during the sampling process. Thus this is a random sample of DATA2002 students.

## What are the potential biases? Which variables are most likely to be subjected to this bias?

Voluntary Response Bias can occur in this research as sample members are self-selected volunteer. 

## Are there any questions that needed improvement to generate useful data?

The questions "How tall are you?" and "What is your shoe size?" shold be improved. Height and shoe size should have a natual limited range. However, In the responses of these questions, every extreme values exist and the standard deviation is quite large. This might because students answered questions with numeric responses in different units. For example, for the height respones less than 2.3, the unit used is obviously "meter" whereas the unit for height respones greater than 100 is centimeter.  When speccifying shoe sizes, students may refer to different sizing system (UK, US, Europe, Modopoint, etc.). 

To improve the questions, units should be specified in the questions so that numeric responses fall in reasonable ranges.The question "How tall are you?" can be changed to "What's your height in centimeter?" while the question "What is your shoe size?" can be changed to "What is your shoe size in UK sizing system?". 

## Does the number of COVID tests follow a Poisson distribution?

```{r}
x %>% janitor::tabyl(covid_test) %>% 
  arrange(covid_test) %>% # arrange in descending order
  gt::gt() %>%
  gt::fmt_percent(columns = c("percent"),
                  decimals = 0) %>%
  gt::tab_source_note("Table: Summary of covid test proportions.")
```

```{r}
x %>% ggplot() + aes(x = covid_test) + 
  geom_bar()

# p1 = x %>% ggplot(aes(x = covid_test)) + geom_bar()
```

There are 8 groups of the number of COVID test: 1,2,3,4,5,6 and 10. The corresponding observed frequencies are 22, 8,3,1,2,1 and 1. $\lambda$ of the Poisson distribution is estimated to be 0.55.

```{r}
tab = as.data.frame(table(x$covid_test))
covid_count = tab$Freq
groups = as.numeric(as.character(tab$Var1))
n=sum(covid_count) # sample size
k = length(covid_count) # number of groups
(lam = sum(covid_count * groups)/n) # estimate the lambda parameter
```

**Hypothesis:** $H_0:$ the data come from a Poisson distribution vs $H_1:$ the data do not come from a Poisson distribution.

**Assumptions:** The expected frequencies, $e_i=np_i \geq 5$. Observations are independent.

The expected frequencies are 79.9, 44.2, 12.3, 2.3, 0.3, 0.03, 0.003 and 0. The expected frequencies $e_4, e_5, e_6, e_7, e_8$ are less than 5, therefore we conbine these 4 classes to satisfy assumption. 
```{r}
p = dpois(groups, lambda = lam) # obtain the p_i from the Poisson omf
(ey = n*round(p,5)) # calculate the expected frequencies
```
```{r}
ey >= 5 # check assumption e_i >= 5 not all satisfied
```

```{r}
(eyr = c(ey[1:3],sum(ey[4:8])))
```


However, after combining the 8 classes into 4 classes (i.e. $0, 1, 2, \geq 3$), the expected frequency of group $\geq 3$ is still less than 5. Therefore we combine the group "2" and "$\geq 3$". The final adjacent groups are "0","1" and "$\geq 2$". The corresponding expected frequencies are 79.9, 44.2 and 14.9 while the observed frequencies are 101, 22,16 respectively. 

```{r}
(eyr = c(ey[1:2],sum(ey[3:8])))
```
```{r}
(yr = c(covid_count[1:2],sum(covid_count[3:8])))
(pr = c(p[1:2],sum(p[3:8])))
```

```{r}
xr = c("0","1",">=3") # group labels
par(mfrow=c(1,2), cex = 1.5)
barplot(yr, names.arg = xr,
        main = "Observed frequency")
barplot(eyr, names.arg = xr,
        main = "Expected frequency")
```

The expected frequencies all are greater than 5 so that the assumption $e_i \geq 5$ is met. As this is a random sampling research, the assumption of independence is met.

**Test statistic:**$T=\sum^k_{i=1}\frac{(Y_i-np_i)^2}{np_i}$.

Under $H_0$, T~$x_2^2$ approximately.

**Observed test statistic:** $t_0=16.86$
```{r}
kr = length(yr) #number of combined classes
(t0 = sum((yr - eyr)^2/eyr))
```

**P-value:** $P(T \geq t_0)=P(x_2^2\geq 16.86)=4e^{-5}$
```{r}
(pval = 1 - pchisq(t0, df = kr-1-1))
```

**Conclusion:** There p-value is $4e^{-5} < 0.05$. The $H_0$ is rejected therefore the number of COVID test follows a Poisson distribution.

## Perform two other hypothesis tests. 

### Are gender and favorite season on excercising independent?
```{r}
x %>% ggplot() + 
  aes(y = fav_season, fill = gender) + 
  geom_bar(position="fill") + 
  labs(y = "", x = "Count")
```
The dimensions of contingency table is $3 \times 4$. In this case, Fisher's exact test can be generalized to $3 \times 4$ two-way contingency tables but is very dificult to compute. Therefore we use Monte Carlo (i.e. random permutation) for the hypothesis testing.

*Frequency table:*
```{r}
(y.mat = table(x$fav_season,x$gender))
```

**Hypothsis:** $H_0:$ gender and favorite season are independent ($p_{ij} =p_ip_j, i=1,2, j=1,2,3,4$) **vs** $H_1:$ gender and favorite season are related (Not all equalities hold).

**Assumptions:** No assumptions are made

```{r}
set.seed(100)
chisq.test(y.mat, simulate.p.value = TRUE, B=10000)
```

**P-value:** p = 0.0124

**Conclusion:** The p-value is 0.0124 < 0.05. We reject $H_0$ and therefore gender and favorite season are related.

### Are gender and favorite social media related?

```{r results=FALSE}
x %>% janitor::tabyl(social_media) %>%
arrange(social_media) %>% # arrange in descending order
  gt::gt() %>% 
  gt::fmt_percent(columns = c("percent"),
                  decimals = 0) %>% 
  gt::tab_source_note("Table 4: Summary of stress level.")

```
```{r}
x1 = x %>% mutate(social_media = case_when(
    social_media == "Youtube" ~ "Youtube",
    social_media == "youtube" ~ "Youtube",
    social_media == "YouTube" ~ "Youtube",
    social_media == "Wechat" ~ "Wechat",
    social_media == "WeChat" ~ "Wechat",
    social_media == "wechat" ~ "Wechat",
    social_media == "twitter" ~ "Twitter",
    social_media == "Twitter" ~ "Twitter",
    social_media == "Tiktok" ~ "Tiktok",
    social_media == "TikTok" ~ "Tiktok",
    social_media == "tiktok" ~ "Tiktok",
    social_media == "Tik Tok" ~ "Tiktok",
    social_media == "Snapchat" ~ "Snapchat",
    social_media == "reddit" ~ "Reddit",
    social_media == "Reddit" ~ "Reddit",
    social_media == "Pinterest" ~ "Pinterest",
    social_media == "no" ~ "None",
    social_media == "None" ~ "None",
    social_media == "Messenger" ~ "Messenger",
    social_media == "messenger" ~ "Messenger", 
    social_media == "linkedin" ~ "Linkedin",
    social_media == "Instagram" ~ "Instagram",
    social_media == "Instragram" ~ "Instagram",
    social_media == "Instangram" ~ "Instagram",
    social_media == "INSTAGRAM" ~ "Instagram",
    social_media == "instagram" ~ "Instagram",
    social_media == "insta" ~ "Instagram",
    social_media == "ins" ~ "Instagram",
    social_media == "Google+" ~ "Google+",
    social_media == "Facebook Messenger" ~ "Messenger",
    social_media == "Facebook messanger" ~ "Messenger",
    social_media == "facebook" ~ "Facebook",
    social_media == "Facebook" ~ "Facebook",
    social_media == "canvas discussion board" ~ "Canvas discussion board",
    social_media == "Club Penguin" ~ "Club Penguin",
    social_media == "Discord" ~ "Discord",
    social_media =="Ed" ~ "Ed",
    social_media =="bilibili" ~ "Bilibili"
  ) ) %>% drop_na()
x1 %>% janitor::tabyl(social_media) %>%
arrange(desc(n)) %>% # arrange in descending order
  gt::gt() %>% 
  gt::fmt_percent(columns = c("percent"),
                  decimals = 0) %>% 
  gt::tab_source_note("Table 4: Summary of favorite social media.")
```
```{r}
x1 %>% ggplot() + 
  aes(y = social_media, fill = gender) + 
  geom_bar(position=position_dodge()) + 
  labs(y = "", x = "Count")
```

The size of contingency table is $3 \times 18$. Many expected frequencies in the contingency table are less than 5 therefore breaking the assumption of Chi-square test. In addition, it is difficult to compute such a large contingency table by Fisher's exact test. Thus, we use an alternative method **Yates' continuity correction** to test in small samples.

*Frequency table:*
```{r}
(y.mat1 = table(x1$social_media,x1$gender))
```

*Contingency table:*
```{r}
r1 = nrow(y.mat1)
c1 = ncol(y.mat1)
yr1 = apply(y.mat1,1,sum) #rowSums(y.mat1)
yc1 = apply(y.mat1,2,sum) #colSums(y.mat1)
yr.mat1 = matrix(yr1, r1, c1, byrow = FALSE)
yc.mat1 = matrix(yc1, r1, c1, byrow = TRUE)
(ey.mat1 = yr.mat1 * yc.mat1 / sum(y.mat1))
```

**Hypothesis:** $H_0:$ gender and favorite social media are independent ($p_{ij}=p_ip_j$ where $i=1,2,3$ and $j=1,2,3,...,18$) **vs** $H_1:$ gender and favorite social media are related (not all equalities hold).

**Assumptions:** Observations are independent.

Under $H_0$, $T$ ~ $\sum^2_{i=1}\sum^2_{j=1}{\frac{(|Y_{ij}-e_{ij}|-0.5)^2}{e_{ij}}}$

**Test statistics:** $t_0=307.3$.

```{r}
res.yates1 = (abs(y.mat1 - ey.mat1)-0.5)^2/ey.mat1
(t0 = sum(res.yates1))
```

**P-value:** $P(T \geq t_0)=P(x_2^2\geq 307.3)=8.3e^{-69} \approx0$

```{r}
pchisq(t0,1,lower.tail=FALSE)
```

**Conclusion:** As p-value < 0.05, we reject $H_0$. Therefore gender and favorite social media are related.

# References

```{r, include = FALSE}
citation("tidyverse")
citation("janitor")
citation("visdat")
citation("gendercodeR")
citation("gt")
citation("skimr")
citation()
```

- Beaudry J, Emily Kothe, Felix Singleton Thorn and Rhydwyn McGuire (2020). gendercodeR: Recodes Sex/Gender Descriptions Into A Standard Set. R package version 0.0.0.9000. https://github.com/ropenscilabs/gendercoder
- Firke S (2020). janitor: Simple Tools for Examining and Cleaning Dirty Data. R package version 2.0.1. https://CRAN.R-project.org/package=janitor
- Iannone R, Joe Cheng and Barret Schloerke (2020). gt: Easily Create Presentation-Ready Display Tables. R package version 0.2.2. https://CRAN.R-project.org/package=gt
- R Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/.
- Tierney N (2017). "visdat: Visualising Whole Data Frames." _JOSS_, *2*(16), 355. doi: [10.21105/joss.00355](https://doi.org/10.21105/joss.00355)
- Waring E, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu and Shannon Ellis (2020). skimr: Compact and Flexible Summaries of Data. R package version 2.1.2. https://CRAN.R-project.org/package=skimr
- Wickham H et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, doi: [10.21105/joss.01686](https://doi.org/10.21105/joss.01686)

