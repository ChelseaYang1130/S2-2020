---
title: "QBUS2820 Assignment2"
author: "SID: 480110301"

# Abstract
#abstract: |
# Paper size for the document, values of letter and a4
papersize: letter

# Font size of the document, values of 9pt (default), 10pt, 11pt and 12pt
fontsize: 12pt

# Optional: Force one-column layout, default is two-column
one_column: true

# Optional: Enables lineo mode, but only if one_column mode is also true
#lineno: true

# Optional: Enable one-sided layout, default is two-sided
#one_sided: true

# Optional: Specify the depth of section number, default is 5
secnumdepth: 3

# Optional: include-after
#include-after: somefile.tex

# Optional: Skip inserting final break between acknowledgements, default is false
skip_final_break: true

# Optional: Bibliography 
bibliography: pinp

# Customize footer, eg by referencing the vignette
footer_contents: "QBUS2820 Assignment 1"

# Produce a pinp document
output: 
  pinp::pinp:
    latex_engine: xelatex

# Required: Vignette metadata for inclusion in a package.
vignette: >
  %\VignetteIndexEntry{YourPackage-vignetteentry}
  %\VignetteKeywords{YourPackage, r, anotherkeyword}
  %\VignettePackage{YourPackage}
  %\VignetteEngine{knitr::rmarkdown}
  
header-includes: 
   \usepackage{wrapfig,subcaption,array,tabularx,multirow,caption} 
   \usepackage[utf8]{inputenc} 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
message = FALSE
warning = FALSE
```

\captionsetup[figure]{labelfont={it,bf,scriptsize},textfont={it,scriptsize},labelsep=colon}
\captionsetup[table]{labelfont={it,bf,scriptsize},textfont={it,scriptsize},labelsep=colon}
\captionsetup[FLOAT_TYPE]{labelformat=simple, labelsep=colon}

# Task A

# Introduction

In a traditional manner, sale prices of houses were predicted by comparing sale prices and costs in the real estate market. There was no general standard to estimate the value of houses. Machine learning techniques therefore play an important role to help establishing models for sale prices of house predictions. As mentioned by Calhoun, the availability of a house price prediction model helps fill up an essential information gap and improve the efficiency of the real estate market (Calhoun, 2003).

This project aims to develop predictive models for sale prices of house with machine learning techniques. With the sale price which is a numerical variable being the response of predictive models, six models are developed and validated.

By comparing the root mean squared errors of predictions, the lasso regression model and random forest model are found to have the best predictive performance for the housing data, compared to elastic net, ridge regression, k-nearest neighbor regression and stepwise regression with forward selection. 

# Data processing and exploratory data analysis

There are 36 numeric variables and 43 categorical variables in the housing data. By calculating the correlation coefficient, 12 numeric variables are found to be potentially linearly related to sale price, as the absolute values of corresponding correlation coefficients are greater than 0.5. The distributions of these variables are visualized in figure \ref{fig:scatter}. 'TotRms AbvGrd', 'Garage Area', '1st Fir SF' and 'SalePrice' are shown to be right-skewed while 'Garage Yr Blt' and 'Overall Qual' are left-skewed, but these distributions are significantly influenced by outliers in several columns. Moreover, some variables, such as 'TotRms AbvGrd', tend to have linear relationships with other variables except sale price, leading to multi-collinearity. This could violate the assumption of some predictive models, such as multiple linear regression, thus robustness to multi-collinearity should be carefully considered when developing predictive models.

Figure \ref{fig:boxplots} shows the distribution of sale price with regard to different categorical features. For most categorical features, sale prices tend to largely different for different groups of the categorical feature, except 'BsmtFin Type 2' and 'Land Slope'. However, although medians sale prices look similar for different groups of 'BsmtFin Type 2' and 'Land Slope', the distribution of sale prices are not identical. Hence, it can still be worthwhile to include these two variables as features to predict sale price. In addition, the boxplots also highlight the outliers of sale price existing in different categorical groups. 

Besides affecting the shapes of data distribution, the existing outliers of numeric variables can also post a significant effect on predictive performance when making sale price predictions.Therefore data pre-processing needs to be considered in the stage of feature engineering in order to overcome issues caused by outliers.

# Feature engineering

\begin{wrapfigure}{r}{0.5\textwidth}
\includegraphics[width=1\linewidth]{miss_plot.png}
\centering
\caption{Visualizing missingness of housing data in training set.}
\label{fig:miss}
\end{wrapfigure}

As shown in Figure \ref{fig:miss}, there are huge amounts of missing values in several columns: 'Alley', 'Fireplace Qu', 'Pool QC', 'Fence', 'Misc Feature', with more than 40% missing values within each column. With this issue, such variables are uninformative to be a feature of predictive models as there are too few observations. To deal with this, removing all rows with missing values can lead to significant loss of information, while imputation using small amount of observations can misrepresent the population for largely incomplete columns. Therefore, 'Alley', 'Pool QC', 'Fence', 'Misc Feature' are abandoned due to high missing rates.

Besides, there are 19 columns containing missing values but the percentages of missing values are less than 20%. This can be deal with by imputation. The missing values are imputed by using the most frequent value of each column.

From EDA analysis, the numeric variables tend to have different scales. Therefore, data standardization is performed prior to model development for numeric variables by subtracting the mean, followed by dividing the standard deviation of the corresponding columns. 

After feature engineering, there are 74 informative features, with 36 features being numeric and 38 features being categorical. There are 1570 observations in the training set and 1210 observations in the testing set. For regression models involving categorical features, dummy variables are created for each categorical feature.

# Methodology

Five regression models are trained by two sets of data where one training set contains numeric features only and the other set includes both categorical and numeric features. With regards to the issue of multi-collinearity mentioned before, all of the five regression techniques used are capable for addressing multi-collinearity. Therefore there is no need to eliminate features suffering from multi-collinearity, and all features surviving in the feature engineering stage are involved in predictive models development.

To develop the best parameter set for each regression models, hyperparameters are tuned with 5-fold cross validation. The performance of models with different values of hyperparameters is estimated by negative mean squared error, where a larger score indicates better predictive performance. For each machine learning model, the value of hyperparameter resulting in the best performance is selected to be optimal.

## Random forest regression

A random forest regression model, which is an extension of decision tree, is developed. It is a supervised learning techniques which applies ensemble learning method for regression. With this technique, decision trees are created in parallel by bagging (i.e. reduce the variance of predictions by resampling). In this case, the mean predicted sale price of the individual trees is reported. 

As random forest applies bootstrap sampling, multi-collinearity is not a concern because it is simply selecting different features from training set to develop models.  In addition, the splits of each tree are randomly sampled from the training set so that with the randomness, overfitting can be avoided.

The number of features being spitted on at each leaf node which is a hyperparameter is the main focus to find the optimal random forest model. Another determinant parameter is the number of trees in random forest. To tun the random forest model, 10 values of  number of trees ('n_estimators'), ranging from 200 to 2000, are fed into models. The maximum number of features being spitted, which is presented as 'max_feature' in python, is obtained by taking a square root of the number of feature.

The optimal number of fetures being splitted is 1200 for the training set with numeric features only. The optimal number changes to 200 when the model involves both categorical and numeric features.

## Lasso Regression

A lasso regression model is developed as it is able to deal with multi-collinearity as well as feature selection. As such it is a highly automate technique with satisfactory predictive performance and interpretability. 

The key component of the lasso model is to perform L1 regularization. Hence the objective is to minimize $\sum^n_{i=1}(y_i-\beta_0-\sum_jx_{ij}\beta_j)^2+\lambda\sum^p_{j=1}|\beta_j|$, where $\lambda$ is the hyperparameter which is tuned to control the strength of L1 regularization penalize. Increasing $\lambda$ results in higher level of L1 penalty and thus more features are eliminated. This also affects the bias-variance tradeoff as a increase of $\lambda$ leads to a increase in bias and a decrease in variance.

It might be challenging to initialize a list of lambdas  to tun the model as there is no strict upper boundary of lambdas. Fortunately, the `LassoCV` package in python can fit the data and automatically find out the optimal $\lambda$ among 100 different values. For the numeric training set, the optimal value of $\lambda$ is 921.2 while it is 130.6 for model including both numeric and categorical features. 

## Ridge Regression

A ridge regression which is similar with the lasso regression, is constructed. In stead of L1 regularization, the ridage regression applies L2 regularization which does not help selecting features but it can still overcome the issue of multi-collinearity.

The hyperparameter is $\lambda$, being similar with that of lasso regression, to minimize
$\sum^n_{i=1}(y_i-\beta_0-\sum_jx_{ij}\beta_j)^2+\lambda\sum^p_{j=1}\beta_j^2$. The `RidgeCV` package in python helps to fit data and select the optimal value of lambda from 100 values. The optimal lambdas for numeric training set and full training set are 113.2 and 9.6 respectively, which are largely different.

## Elastic Nets

An elastic nets model performs both L1 and L2 regularization, and integrates the strength of ridge regression and lasso regression. As such, it is able to perform certain level of feature selection as well as placing no restriction on the numer of selected variables. 

The hyperparameter $\lambda$ is tuned to minimize $\sum^n_{i=1}(y_i-\beta_0-\sum_jx_{ij}\beta_j)^2+\lambda\sum^p_{j=1}(\alpha \beta_j^2+(1-\alpha)|\beta_j|)$. Being similar to `RidgeCV` and `LassoCV`, the `ElasticNetCV` package in python selects the optimal value of $\lambda$ from 100 different values for the fitted data. The optimal lambdas are both 65.6 for numeric set and full set.

## Stepwise regression 

The stepwise regression is simply a multiple linear regression with feature selection. The farward selection approach starts from a null model which contains only the constant, and iterate to add different number of features to the model. For each iteration, all predictors are add individually to the model to construct $p-k-1$ models, where $p$ is the total number of predictors available, and $k$ is the number of predictors involved in models. For a specific value of $k$, the best model is selected based on residual sum of square (RSS). Finally, the optimal stepwise model is selected by comparing the negative root mean squred error from cross validation, in order to estimate predictive performance of models involving different number of predictors.

The optimal model with numeric features contains 17 predictors while the best model including both categorical and numeric variables contains 73 features.

\hspace{1cm}

\begin{wraptable}{r}{8cm}
\begin{tabular}{ |c|c|c| } 
\hline
\textbf{Model / Features} & \textbf{Numeric} & \textbf{Numeric and Categorical} \\
\hline
\textbf{Forward stepwise} & 34470.77 & $5.21\times 10^{15}$ \\ 
\textbf{Lasso} & 34543.61 & 29628.07 \\
\textbf{Ridge} & 34475.68 & 30082.79 \\
\textbf{Elastic net} & 35380.58 & 32844.26 \\
\textbf{Random forest} & 28016.33 & 28805.04\\
\hline
\end{tabular}
\centering
\caption{Summary of predictive performance for regression models. The performance assessement metric is the root mean squared error from cross validation.}
\label{table:cv_errors}
\end{wraptable}

The predictive performance of models developed are validated by root mean squared error (RMSE) from 5-fold cross validation. Table \ref{table:cv_errors} summaries the predictive performances of five regression models for the two training sets. For models including only numeric features, random forest regressio has the best performance with the lowest RMSE while other 4 models have similar peroformance. For models with both numeric and categorical variables, random forest regression still has the most accurate predictions, followed by lasso regression. 

# Validation set results from kaggle

\begin{wraptable}{r}{8cm}
\begin{tabular}{ |c|c|c| } 
\hline
\textbf{Model / Features} & \textbf{Numeric} & \textbf{Numeric and Categorical} \\
\hline
\textbf{Forward stepwise} & 40672.34  & 46984.44 \\ 
\textbf{Lasso} & 39910.50 & 29628.07 \\
\textbf{Ridge} & 40141.87 & 38481.91 \\
\textbf{Elastic net} & 38249.77 &  37679.12\\
\textbf{Random forest} & 25405.28 & 28276.96\\
\hline
\end{tabular}
\centering
\caption{Summary of validation set results from kaggle. The performance assessement metric is the root mean squared error from cross validation.}
\label{table:kaggle_results}
\end{wraptable}

Table \ref{table:kaggle_results} summirizes the validation set results from kaggle, assessing the predictive performance of five regression models by root mean squared errors. It is inline with table \ref{table:cv_errors} that random forest and lasso model regression have the best predictive performance for both training sets. Moreover, random forest tends to have more accurate predictions for numeric sets compared to models with both categorical and numeric variables.

# Conclusion

It is noticable from table \ref{table:cv_errors} that the stepwise regression with forward selection has a poor predictions with a very large RMSE for training set involving categorical variables, whereas the predictive performance for numeric set is relatively satisfactory. This is because when perforaming stepwise regression, the dummy variables representing categorical variables are treated as numeric variable when fitting multiple linaer regression. 

# References

- Gibson, M., Little, R. and Rubin, D., 1989. Statistical Analysis with Missing Data. The Statistician, 38(1), p.82.
- Scikit-learn.org. 2020. 6.4. Imputation Of Missing Values — Scikit-Learn 0.23.2 Documentation. [online] Available at: <https://scikit-learn.org/stable/modules/impute.html>.
- Scikit-learn.org. 2020. 6.4. Imputation Of Missing Values — Scikit-Learn 0.23.2 Documentation. [online] Available at: <https://scikit-learn.org/stable/modules/impute.html>.
- Hintze, J.L., 1992. Chapter 335: Ridge Regression. In Number cruncher statistical system: statistical software. Kaysville, UT: Jerry L. Hintze. 
- Chakon, O. (2017). Practical Machine Learning: Ridge Regression Vs Lasso. Coding Startups: Coders With Entrepreneurial Mindset. Published August 3rd, 2017.
- Allison, P. (2012). When Can You Safely Ignore Multicollinearity? Statistical Horizons.
- Cross Validated. (2015). What is elastic net regularization, and how does it solve the drawbacks of Ridge (L2) and Lasso (L1)? [online] Available at: https://stats.stackexchange.com/questions/184029/what-is-elastic-net-regularization-and-how-doesitsolve-the-drawbacks-of-ridge/184031#184031.

# Task B

# Exploratory data analysis

\begin{wrapfigure}{r}{0.5\textwidth}
\includegraphics[width=1\linewidth]{decomposition.png}
\centering
\caption{Time series decomposition of number of visitors from 1991 to 2016.}
\label{fig:timeseries}
\end{wrapfigure}

The time series decomposition figure \ref{fig:timeseries} shows an upward trend from 1991 to 2016, with a seasonal pattern as systematic changes occur in short periods which are fixed. Furthermore, the variation of number of visitors within the fixed period becomes greater as time moves. As such, a multiplicative forecasting model may be more suitable for this data compared to an additive model.

# Forecasting models

To capture both the trend and seasonal pattern of number of visitors, two exponential smoothing, Holt-Winters and Daped trend, are developed.

In order to capture both the upward trend and seasonal pattern of the number of visitors, a Holt-Winters exponential smoothing model is developed.

## Holt-Winters exponential smoothing 

\begin{wrapfigure}{r}{0.5\textwidth}
\includegraphics[width=1\linewidth]{HW_es.png}
\centering
\caption{Smoothed time series by Holt-Winter exponential smoothing.}
\label{fig:hw_es}
\end{wrapfigure}

As an extension of the Holt exponential smoothing, the Holt-Winters exponential smoothing capture not only the trend but also the seasonal pattern of number of visitors. It is available for both additive and multiplicative models. In this case, as mentioned before, the variance of number of visitors increases as time goes by, a multiplicative model is considered.

The model involves three paramters $l_t,b_t$ and $S_t$ which represent the level, trend and seasonal indices respectivey for the time series of number of visitors. The forecast equation of the multiplicative model is $\hat y_{t+h}=(\hat l_t+h\hat b_t \times S_{t-L+(h~mod~L)}$ where mod is the modulo operator, $I_{i,L}=0$ if $h~mod~L\neq i$ and $I_{i,L}=1$ if $h~mod~L=i$.

The estimated paramters of Holt-winters exponential smoothing are $\alpha=0.31,\beta=0.012,\delta=0.362$ for this data.

### Model diagnostics

\begin{figure}
\includegraphics[width=0.8\textwidth]{diagnostic_plot.png}
\centering
\caption{Residual plot, residual ACF, residual distribution plot and normal Q-Q plot for Holt-Winter exponential smoothing model.}
\label{fig:diagnostic_plot}
\end{figure}

## Model validation

## Forecasting results


# Appendix

\begin{figure}
\includegraphics[width=0.8\textwidth]{scatter.png}
\centering
\caption{Distribution of numeric variables in housing data.}
\label{fig:scatter}
\end{figure}

\begin{figure}
\includegraphics[width=0.8\textwidth]{boxplot.png}
\centering
\caption{Boxplots demonstrating distribution of sale price for houses with different categorical features in housing data.}
\label{fig:boxplots}
\end{figure}